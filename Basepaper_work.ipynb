{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Pud-3ookpnP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as TF\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "from google.colab import files\n",
        "from zipfile import ZipFile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "image_dir = '/content/drive/My Drive/UOD/DUO/train'"
      ],
      "metadata": {
        "id": "DKcuh2DTkwr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_image_10\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "image_dir = '/content/drive/My Drive/UOD/DUO/train_image_10'\n",
        "ground_image_dir = '/content/drive/My Drive/UOD/DUO/enhanced_image_10'"
      ],
      "metadata": {
        "id": "mcn6FGi8k11N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**JOINT COMMON ENCODER:**"
      ],
      "metadata": {
        "id": "4k6ybumVk7MM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CommonEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CommonEncoder, self).__init__()\n",
        "\n",
        "        self.C1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.C2 = self._make_residual_level(in_channels=64, out_channels=64, stride=4)\n",
        "        self.C3 = self._make_residual_level(in_channels=64, out_channels=64, stride=8)\n",
        "        self.C4 = self._make_residual_level(in_channels=64, out_channels=64, stride=16)\n",
        "        self.C5 = self._make_residual_level(in_channels=64, out_channels=64, stride=32)\n",
        "\n",
        "        '''self.C1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.C2 = self._make_residual_level(in_channels=64, out_channels=64, stride=1)\n",
        "        self.C3 = self._make_residual_level(in_channels=64, out_channels=64, stride=2)\n",
        "        self.C4 = self._make_residual_level(in_channels=64, out_channels=64, stride=4)\n",
        "        self.C5 = self._make_residual_level(in_channels=64, out_channels=64, stride=8)'''\n",
        "\n",
        "    def _make_residual_level(self, in_channels, out_channels,stride):\n",
        "        layers = []\n",
        "        for _ in range(4):\n",
        "            layers.append(ResidualBlock(in_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.C1(x)\n",
        "        c2_out = self.C2(x)\n",
        "        c3_out = self.C3(c2_out)\n",
        "        c4_out = self.C4(c3_out)\n",
        "        c5_out = self.C5(c4_out)\n",
        "        return c2_out,c3_out,c4_out,c5_out\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,stride=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        #x += residual\n",
        "        x = self.relu(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "YAfmRj4Ik8Rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvolutionBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ConvolutionBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.conv1(x))\n",
        "        out = self.relu(self.conv2(out))\n",
        "        out = self.relu(self.conv3(out))\n",
        "        return out\n",
        "\n",
        "class CommonEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CommonEncoder, self).__init__()\n",
        "        self.C1 = ConvolutionBlock(in_channels=3, out_channels=64)\n",
        "        self.C2 = ConvolutionBlock(in_channels=64, out_channels=64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.C1(x)\n",
        "        out = self.C2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "VwAtkznXk-O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FEATURE RECONSTRUCTION BLOCK:**"
      ],
      "metadata": {
        "id": "W4s1xjKMlEPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureReconstructionBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(FeatureReconstructionBlock, self).__init__()\n",
        "\n",
        "        # Feature reconstruction block\n",
        "        self.conv0 = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        self.conv1 = nn.Conv2d(out_channels, out_channels, kernel_size=1)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(out_channels * 2, out_channels, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(out_channels * 2, out_channels, kernel_size=5, padding=2)\n",
        "        self.conv5 = nn.Conv2d(out_channels * 2, out_channels, kernel_size=3, padding=1)\n",
        "        self.conv6 = nn.Conv2d(out_channels * 2, out_channels, kernel_size=7, padding=3)\n",
        "        self.conv7 = nn.Conv2d(out_channels * 4, 3 , kernel_size=3, padding=1)\n",
        "\n",
        "        '''self.conv0 = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        self.conv1 = nn.Conv2d(out_channels, out_channels, kernel_size=1)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3)\n",
        "        self.conv3 = nn.Conv2d(out_channels * 2, out_channels, kernel_size=3)\n",
        "        self.conv4 = nn.Conv2d(out_channels * 2, out_channels, kernel_size=5)\n",
        "        self.conv5 = nn.Conv2d(out_channels * 2, out_channels, kernel_size=3)\n",
        "        self.conv6 = nn.Conv2d(out_channels * 2, out_channels, kernel_size=7)\n",
        "        self.conv7 = nn.Conv2d(out_channels * 4, 3 , kernel_size=3)'''\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Feature reconstruction\n",
        "        conv0_out = self.conv0(x)\n",
        "        conv1_out = self.conv1(conv0_out)\n",
        "        conv2_out = self.conv2(conv1_out)\n",
        "        conv3_concat = self.conv3(torch.cat((conv1_out, conv2_out), dim=1))\n",
        "        conv4_out = self.conv4(torch.cat((conv2_out, conv3_concat), dim=1))\n",
        "        conv5_concat = self.conv5(torch.cat((conv2_out, conv4_out), dim=1))\n",
        "        conv6_out = self.conv6(torch.cat((conv4_out, conv5_concat), dim=1))\n",
        "        conv7_concat = self.conv7(torch.cat((conv1_out, conv2_out, conv4_out, conv6_out), dim=1))\n",
        "\n",
        "        return conv7_concat\n"
      ],
      "metadata": {
        "id": "ibhJPxrHlAqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**COLOR ADJUSTMENT BLOCK:**"
      ],
      "metadata": {
        "id": "32Jt5gt7lJpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ColorAdjustmentBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "        super(ColorAdjustmentBlock, self).__init__()\n",
        "\n",
        "        self.color_conv1 = nn.Conv2d(3 , out_channels, kernel_size=3, padding=1)\n",
        "        self.color_conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.color_conv3 = nn.Conv2d(out_channels, 3 , kernel_size=3, padding=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "        #color_out = self.color_conv1(torch.cat((conv1_out, conv2_out, conv4_out, conv6_out), dim=1))\n",
        "        color_out= self.color_conv1(x)\n",
        "        color_out = self.relu(color_out)\n",
        "        color_out = self.color_conv2(color_out)\n",
        "        color_out = self.relu(color_out)\n",
        "        color_out = self.color_conv3(color_out)\n",
        "        color_out = self.relu(color_out)\n",
        "\n",
        "        output = self.sigmoid(color_out)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "gRq9MA8ulI-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LOSS FUNCTION**"
      ],
      "metadata": {
        "id": "dVIIQ-5GlgAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomLoss, self).__init__()\n",
        "\n",
        "    def forward(self, c1_lambda, c_lambda, m_lambda, m_g_lambda):\n",
        "        # Pixel-wise Mean Square Error (MSE) Loss\n",
        "        mse_loss = F.mse_loss(c_lambda, c1_lambda)\n",
        "\n",
        "        # Contrast Adjustment Loss\n",
        "        contrast_loss = torch.norm(m_lambda - m_g_lambda, p=2)\n",
        "\n",
        "        # Gradient Loss\n",
        "        grad_m_lambda = torch.autograd.grad(contrast_loss, m_lambda, create_graph=True)[0]\n",
        "        grad_m_g_lambda = torch.autograd.grad(contrast_loss, m_g_lambda, create_graph=True)[0]\n",
        "\n",
        "        # Squared 2-norm of the element-wise absolute difference\n",
        "        gradient_loss = torch.norm(torch.abs(grad_m_lambda) - torch.abs(grad_m_g_lambda), p=2)\n",
        "\n",
        "        # Total Loss\n",
        "        total_loss = mse_loss + contrast_loss + gradient_loss\n",
        "\n",
        "        return total_loss"
      ],
      "metadata": {
        "id": "qaxe88bfljX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = CommonEncoder()\n",
        "feature_recons = FeatureReconstructionBlock(in_channels=64, out_channels=64)\n",
        "color_adjus = ColorAdjustmentBlock(in_channels=256, out_channels=256)\n",
        "loss_function = CustomLoss()\n",
        "optimizer = torch.optim.Adam(encoder.parameters(), lr=0.001)\n",
        "\n",
        "if not os.path.exists(image_dir):\n",
        "    print(f\"Directory {image_dir} does not exist.\")\n",
        "else:\n",
        "    image_files = [filename for filename in os.listdir(image_dir) if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\")]\n",
        "\n",
        "    for epoch in range(100):\n",
        "        total_loss = 0  # Initialize total loss for the epoch\n",
        "\n",
        "        for idx, filename in enumerate(image_files[:4]):\n",
        "            image_path = os.path.join(image_dir, filename)\n",
        "            input_image = Image.open(image_path)\n",
        "            # No data transformation for input image\n",
        "            input_tensor = transforms.ToTensor()(input_image).unsqueeze(0)\n",
        "            #input_tensor = F.interpolate(input_tensor, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "            print(f\"input shape for {filename}: {input_tensor.shape}\")\n",
        "\n",
        "            c2,c3,c4,c5 = encoder(input_tensor)\n",
        "            print(f\"common encoder Output shape for {filename}: {c2.shape}\")\n",
        "            recons_output = feature_recons(c2)\n",
        "            print(f\"Feature Reconstruction Output shape for {filename}: {(recons_output).shape}\")\n",
        "            recons_output = F.interpolate(recons_output, size=(640, 640), mode='bilinear', align_corners=False)\n",
        "            colorAdj_output = color_adjus(recons_output)\n",
        "            print(f\"Color Adjustment Output shape for {filename}: {(colorAdj_output).shape}\")\n",
        "\n",
        "            # Calculate c_lambda\n",
        "            #recons_output = F.interpolate(recons_output, size=(640, 640), mode='bilinear', align_corners=False)\n",
        "            c_lambda = (recons_output * input_tensor) - recons_output + 1\n",
        "\n",
        "            enhanced_output = c_lambda * colorAdj_output\n",
        "            #enhanced_output = c_lambda  - black with green color\n",
        "            #enhanced_output = colorAdj_output - grey color\n",
        "\n",
        "            ground_truth_image_path = os.path.join(ground_image_dir, filename)\n",
        "            ground_truth_image = Image.open(ground_truth_image_path)\n",
        "            # No data transformation for ground truth image\n",
        "            ground_truth_tensor = transforms.ToTensor()(ground_truth_image).unsqueeze(0)\n",
        "            #ground_truth_tensor = F.interpolate(ground_truth_tensor, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "            print(f\"gt shape for {filename}: {ground_truth_tensor.shape}\")\n",
        "\n",
        "            c2_g,c3_g,c4_g,c5_g = encoder(ground_truth_tensor)\n",
        "            recons_output_gt = feature_recons(c2_g)\n",
        "\n",
        "            recons_output_gt = F.interpolate(recons_output_gt, size=(640, 640), mode='bilinear', align_corners=False)\n",
        "            loss = loss_function(ground_truth_tensor, c_lambda, colorAdj_output, recons_output_gt)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Calculate average loss for the epoch\n",
        "        average_loss = total_loss / len(image_files)\n",
        "\n",
        "        print(f'Epoch {epoch + 1}, Average Loss: {average_loss}')\n",
        "\n",
        "        # Perform optimization once after processing all images in the epoch\n",
        "        optimizer.zero_grad()\n",
        "        #torch.tensor(average_loss, requires_grad=True).backward()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "MgDu0tUZmB3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_image = TF.to_pil_image(input_tensor.squeeze())\n",
        "enhanced_image = TF.to_pil_image(enhanced_output.squeeze().detach())\n",
        "\n",
        "# Display the images\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "axes[0].imshow(input_image)\n",
        "axes[0].set_title('Input Image')\n",
        "axes[1].imshow(enhanced_image)\n",
        "axes[1].set_title('Enhanced Image')"
      ],
      "metadata": {
        "id": "Pl7tOlVSlkRN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}